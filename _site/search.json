[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks/Invited Lectures",
    "section": "",
    "text": "Invited Lectures\n\nData Analysis using R (Hands-on Experience)\nPresidency University, Bangalore (Online, 2024)\nDelivered a session on using R for data analysis as part of the Faculty Development Program on Advanced Research Methodology & Data Analysis.\n🔗 Slides available here\nUsing R for Data Analysis\nCentral University of South Bihar (Gaya, Bihar, 2022)\nInvited lecture to introduce R for social science researchers.\n\n\n\nTalks\n\nWhat Grad School Don’t Teach You: Tools for Transparent and Reproducible Research\nPreconference Workshop, KEA Economic Conference 2025\nDelivered a session on the importance of reproducibility in research, covering tools like Git, Quarto, R, and Python.\n🔗 Materials available here\nPurrr, A Multipurpose Package\nBoston R Users Group (Remote, 2021)\nDelivered a lightning talk on the use of one of my favorite packages, ‘Purrr.’\nA Sneak Peek into R for Economics\nEconomiga (Remote, 2021)\nDelivered a live talk on using R in Economics Research.\n🔗 Watch the talk here\n🔗 Event details"
  },
  {
    "objectID": "new.html",
    "href": "new.html",
    "title": "Generate images with OpenAI",
    "section": "",
    "text": "In this tutorial, we will explore how to generate images using OpenAI’s image generation API in R. The openai package provides a simple way to interact with OpenAI’s models, making it easy for R users to create AI-generated visuals. Whether you’re a data scientist, researcher, or hobbyist, this guide will help you get started with generating images programmatically."
  },
  {
    "objectID": "new.html#install-and-load-the-openai-package",
    "href": "new.html#install-and-load-the-openai-package",
    "title": "Generate images with OpenAI",
    "section": "Install and Load the openai Package",
    "text": "Install and Load the openai Package\nBefore we begin, make sure you have the openai package installed. If you haven’t installed it yet, you can do so using:\ninstall.packages(\"openai\")\nLoad the package into your R session:\nlibrary(openai)"
  },
  {
    "objectID": "new.html#setting-up-your-api-key",
    "href": "new.html#setting-up-your-api-key",
    "title": "Generate images with OpenAI",
    "section": "Setting Up Your API Key",
    "text": "Setting Up Your API Key\nTo use OpenAI’s API, you need an API key. If you haven’t already, sign up on OpenAI’s websiteand obtain your API key. Then, store it securely in your R session:\nSys.setenv(OPENAI_API_KEY = \"your-api-key-here\")\nAlternatively, you can store the key in an .Renviron file for persistent access."
  },
  {
    "objectID": "new.html#basic-image-generation",
    "href": "new.html#basic-image-generation",
    "title": "Generate images with OpenAI",
    "section": "Basic Image Generation",
    "text": "Basic Image Generation\nHere’s a simple example where we generate an image of a futuristic city:\nresponse &lt;- create_image(\n  prompt = \"A futuristic city skyline at sunset with flying cars\",\n  n = 1,  # Number of images to generate\n  size = \"1024x1024\"  # Image size\n)\n\n# Print the response to get the image URL\nresponse$data$url"
  },
  {
    "objectID": "new.html#customizing-the-output",
    "href": "new.html#customizing-the-output",
    "title": "Generate images with OpenAI",
    "section": "Customizing the Output",
    "text": "Customizing the Output\nYou can modify the n parameter to generate multiple images at once:\nresponse &lt;- create_image(\n  prompt = \"A cyberpunk-themed street with neon lights\",\n  n = 3,  # Generate three images\n  size = \"1024x1024\"\n)\n\n# View all generated image URLs\nresponse$data$url"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Nithin M\nSocial Services Division\nKerala State Planning Board\nPattom, Thiruvananthapuram\nKerala - 695004\nPhone: +91 9447748013\nE-mail: write2nithinm@gmail.com"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my Blog!!\nHere, I’ll share ideas, tips, and tricks on R programming, Statistics, Econometrics, Data Science, and a bit about running. Whether you’re interested in data or looking for running motivation, I hope you find something useful!\nPlease feel free to reach out to me!!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGenerate images with OpenAI\n\n\n\n\n\n\nR\n\n\nOpenAI\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs in R and Python\n\n\n\n\n\n\nR\n\n\nPython\n\n\nAI\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nNithin M\n\n\n\n\n\n\n\n\n\n\n\n\nStreamlining R Workflows: A Practitioner’s Guide to Data Management\n\n\n\n\n\n\nR\n\n\nData Management\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Python vs R\n\n\n\n\n\n\nR\n\n\nPython\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJun 7, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/AI/index.html",
    "href": "blog/AI/index.html",
    "title": "Generate images with OpenAI",
    "section": "",
    "text": "In this tutorial, we will explore how to generate images using OpenAI’s image generation API in R. The openai package provides a simple way to interact with OpenAI’s models, making it easy for R users to create AI-generated visuals. Whether you’re a data scientist, researcher, or hobbyist, this guide will help you get started with generating images programmatically."
  },
  {
    "objectID": "blog/AI/index.html#install-and-load-the-openai-package",
    "href": "blog/AI/index.html#install-and-load-the-openai-package",
    "title": "Generate images with OpenAI",
    "section": "Install and Load the openai Package",
    "text": "Install and Load the openai Package\nBefore we begin, make sure you have the openai package installed. If you haven’t installed it yet, you can do so using:\n\ninstall.packages(\"openai\", repos = \"https://cloud.r-project.org/\")\n\nLoad the package into your R session:\n\nlibrary(openai)"
  },
  {
    "objectID": "blog/AI/index.html#setting-up-your-api-key",
    "href": "blog/AI/index.html#setting-up-your-api-key",
    "title": "Generate images with OpenAI",
    "section": "Setting Up Your API Key",
    "text": "Setting Up Your API Key\nTo use OpenAI’s API, you need an API key. If you haven’t already, sign up on OpenAI’s websitea and obtain your API key. Then, store it securely in your R session:\n\nSys.setenv(OPENAI_API_KEY = \"Your API Key\")\n\nAlternatively, you can store the key in an .Renviron file for persistent access."
  },
  {
    "objectID": "blog/AI/index.html#basic-image-generation",
    "href": "blog/AI/index.html#basic-image-generation",
    "title": "Generate images with OpenAI",
    "section": "Basic Image Generation",
    "text": "Basic Image Generation\nHere’s a simple example where we generate an image of a futuristic city:\n\nresponse &lt;- create_image(\n  prompt = \"a roman cat\",\n  n = 1,  # Number of images to generate\n  size = \"1024x1024\"  # Image size\n)\n\n# Print the response to get the image URL\nresponse$data$url"
  },
  {
    "objectID": "blog/AI/index.html#customizing-the-output",
    "href": "blog/AI/index.html#customizing-the-output",
    "title": "Generate images with OpenAI",
    "section": "Customizing the Output",
    "text": "Customizing the Output\nYou can modify the n parameter to generate multiple images at once:\n\nresponse &lt;- create_image(\n  prompt = \"A cyberpunk-themed street with neon lights\",\n  n = 3,  # Generate three images\n  size = \"1024x1024\"\n)\n\n# View all generated image URLs\nresponse$data$url"
  },
  {
    "objectID": "blog/Data/index.html",
    "href": "blog/Data/index.html",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "",
    "text": "If the first line of your R script is\nsetwd(\"C:\\\\Users\\\\jenny\\\\path\\\\that\\\\only\\\\I\\\\have\")\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n\n\nIf the first line of your R script is\nrm(list = ls())\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n\n— Jenny Bryan\nData management is a crucial aspect of any data analysis workflow in R. Whether you are a beginner or a seasoned practitioner, effectively renaming variables and managing your datasets can significantly influence your productivity and the clarity of your analysis.\nIn this guide, we’ll explore common methods that users typically employ to rename variables and create labels in R, followed by a more structured and practical approach that can enhance efficiency and maintainability in your data management practices.\n\n\nBefore we dive into data management techniques, it’s essential to organize your work efficiently. I recommend starting by creating a new project in RStudio. This will help you manage your files and set a working directory automatically.\n\nOpen RStudio.\nGo to File -&gt; New Project....\nChoose New Directory -&gt; New Project.\nName your project and select a directory.\nClick Create Project.\n\nWith your project set up, you can easily manage your files and scripts, ensuring that your working directory is always pointing to the correct folder.\n\n\n\nLet us create a small sample data\n\n# Sample data frame\ndata &lt;- data.frame(\n  id = 1:5,\n  age = c(21, 25, 30, 22, 28),\n  gender = factor(c(\"Male\", \"Female\", \"Male\", \"Female\", \"Female\")),\n  income = c(50000, 60000, 70000, 80000, 90000),\n  education = c(1, 2, 3, 2, 1) # Numeric variable for education\n)\n\n\nlibrary(here) # for relative path\n# Save the dataset using the here package\nwrite_csv(data, file = here(\"data\", \"dummy_data.csv\"))\n\nThis will store our dummy data in the project folder."
  },
  {
    "objectID": "blog/Data/index.html#setting-up-your-rstudio-project",
    "href": "blog/Data/index.html#setting-up-your-rstudio-project",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "",
    "text": "Before we dive into data management techniques, it’s essential to organize your work efficiently. I recommend starting by creating a new project in RStudio. This will help you manage your files and set a working directory automatically.\n\nOpen RStudio.\nGo to File -&gt; New Project....\nChoose New Directory -&gt; New Project.\nName your project and select a directory.\nClick Create Project.\n\nWith your project set up, you can easily manage your files and scripts, ensuring that your working directory is always pointing to the correct folder."
  },
  {
    "objectID": "blog/Data/index.html#sample-data",
    "href": "blog/Data/index.html#sample-data",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "",
    "text": "Let us create a small sample data\n\n# Sample data frame\ndata &lt;- data.frame(\n  id = 1:5,\n  age = c(21, 25, 30, 22, 28),\n  gender = factor(c(\"Male\", \"Female\", \"Male\", \"Female\", \"Female\")),\n  income = c(50000, 60000, 70000, 80000, 90000),\n  education = c(1, 2, 3, 2, 1) # Numeric variable for education\n)\n\n\nlibrary(here) # for relative path\n# Save the dataset using the here package\nwrite_csv(data, file = here(\"data\", \"dummy_data.csv\"))\n\nThis will store our dummy data in the project folder."
  },
  {
    "objectID": "blog/Data/index.html#dropping-some-variables",
    "href": "blog/Data/index.html#dropping-some-variables",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Dropping some variables",
    "text": "Dropping some variables\nDropping variables is a key operation in data cleaning. Let us see how we do it\n\nBase-RTidyverse\n\n\n\n# Drop 'education' column by excluding it\ndata &lt;- data[, names(data) != \"education\"]\n\n\n# Alternatively, keep only specific columns\ndata &lt;- data[, c(\"id\", \"age\", \"gender\", \"income\")]\n\n\n\n\nlibrary(dplyr)\n\n# Drop 'education' column using dplyr\ndata &lt;- data %&gt;% select(-education)\n\n# Keep only specific columns\ndata &lt;- data %&gt;% select(id, age, gender, income)"
  },
  {
    "objectID": "blog/Data/index.html#renaming",
    "href": "blog/Data/index.html#renaming",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Renaming",
    "text": "Renaming\nRenaming variables is essential for clarity and consistency, especially when dealing with raw data that may use cryptic or inconsistent naming conventions, long variable names etc.\nMany users tend to rely on base R functions to rename variables within their data frames. The most common method involves using the names() function or colnames() function to directly set new variable names. Here’s how typical users might approach this task:\n\ncolnames(data) &lt;- c(\"respondent_id\", \"age\", \"gender\", \"income\", \"education_level\")\nhead(data)\n\na more familiar user of R might prefer using tidyverse approach which can be of two methods\nUsing the dplyr package, you can rename variables in two ways: with the rename() function or by using the select() function. Below is an example of both approaches\n\nUsing rename()Using select()\n\n\n\n# Load dplyr\nlibrary(dplyr)\n\n# Sample data frame\ndata &lt;- data.frame(\n  id = 1:5,\n  age = c(21, 25, 30, 22, 28),\n  gender = factor(c(\"Male\", \"Female\", \"Male\", \"Female\", \"Female\")),\n  income = c(50000, 60000, 70000, 80000, 90000),\n  education = c(1, 2, 3, 2, 1) # Numeric variable for education\n)\n\n# Renaming variables using rename()\ndata &lt;- data %&gt;%\n  rename(\n    respondent_id = id,\n    education_level = education\n  )\n\n# Display updated names\nhead(data)\n\n\n\n\n# Load dplyr\nlibrary(dplyr)\n\n# Sample data frame\ndata &lt;- data.frame(\n  id = 1:5,\n  age = c(21, 25, 30, 22, 28),\n  gender = factor(c(\"Male\", \"Female\", \"Male\", \"Female\", \"Female\")),\n  income = c(50000, 60000, 70000, 80000, 90000),\n  education = c(1, 2, 3, 2, 1) # Numeric variable for education\n)\n\n# Renaming variables using select()\ndata &lt;- data %&gt;%\n  select(\n    respondent_id = id,\n    age,\n    gender,\n    income,\n    education_level = education\n  )\n\n# Display updated names\nhead(data)"
  },
  {
    "objectID": "blog/Data/index.html#labelling",
    "href": "blog/Data/index.html#labelling",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Labelling",
    "text": "Labelling\nVariable labels provide descriptive metadata that help users (including future you!) understand the meaning of each variable. This is especially useful in large projects or when preparing data for sharing. R users often overlook the importance of creating labels for their variables. When they do decide to create labels, they typically use the labelled package or similar approaches. Here’s how they might go about it:\n\nBase R (with Hmisc)Tidyverse (with labelled)\n\n\n\n# install.packages(\"Hmisc\") # if not already installed\nlibrary(Hmisc)\n\nlabel(data$age) &lt;- \"Age in years\"\nlabel(data$gender) &lt;- \"Gender of respondent\"\nlabel(data$income) &lt;- \"Annual income in USD\"\nlabel(data$education) &lt;- \"Education level\"\n\n\n\n\n# install.packages(\"labelled\") # if not already installed\nlibrary(labelled)\n\ndata &lt;- data %&gt;%\n  set_variable_labels(\n    age = \"Age in years\",\n    gender = \"Gender of respondent\",\n    income = \"Annual income in USD\",\n    education = \"Education level (1=Primary, 2=Secondary, 3=Tertiary)\"\n  )"
  },
  {
    "objectID": "blog/Data/index.html#setting-up-the-environment",
    "href": "blog/Data/index.html#setting-up-the-environment",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\nThe first step in creating a reproducible working environment in R is setting up your packages and dependencies in a clean, consistent manner.\n\nWhy pacman?\nOver the years, I’ve tried various strategies to streamline package management — including writing custom functions to install and load required libraries. These functions usually looped through a list of package names, checked if they were installed, installed those that were missing, and then loaded them. While this worked, it was often verbose and harder to maintain over time.\nNow, I prefer using pacman, which offers the same functionality with a cleaner, more robust interface.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse, haven, janitor, labelled, skimr,\n  readxl, writexl, here, stringr\n)\n\nThis one-liner installs any missing packages and loads them, eliminating the need to manually manage install.packages() and library() calls — and avoids the common practice of commenting out installation lines, which can silently break code for others or even your future self.\nMy Earlier Approach Before adopting pacman, I wrote a custom function to do just this — install and load a list of packages in a single go. You can view that function here:\nCustom Package Loader Function (GitHub Gist)\n\n\nBeyond pacman: Full Environment Reproducibility\nFor long-term and collaborative projects, it’s beneficial to complement pacman with tools that enhance reproducibility and robustness — such as renv, targets, and Docker.\nThese tools help manage dependencies, structure workflows, and containerize your environment for seamless collaboration and future-proofing. I’ll discuss each of them in more detail later."
  },
  {
    "objectID": "blog/Data/index.html#rename-drop-label-the-smart-way-with-data-dictionaries",
    "href": "blog/Data/index.html#rename-drop-label-the-smart-way-with-data-dictionaries",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Rename, Drop, Label — the Smart Way with Data Dictionaries",
    "text": "Rename, Drop, Label — the Smart Way with Data Dictionaries\nA data dictionary — basically a tidy rectangular table that lists variable names, their definitions, labels, and other attributes — is one of the most important things you’ll ever create in your research workflow.\nMost people think of it as something you make at the end, just to help interpret your dataset. But trust me, it’s way more useful if you start building it before you even collect or clean your data. It acts like a roadmap — helping you stay organized from raw data all the way to your final cleaned dataset.\nWith this approach:\n\nYou won’t lose track of how or why you renamed a variable.\nYou’ll always know which variables were dropped and what the logic was.\nYou’ll have consistent, clear labels — no guesswork later on.\n\nIt’s not just about reproducibility (though that’s huge). It’s also about clarity — for your collaborators, and more importantly, your future self. This way, your data workflow becomes transparent, structured, and way easier to manage as your project grows.\n\nIntegrating Data Dictionaries into Your Workflow\nData dictionaries are ideally created at the beginning of a project — they act as a roadmap for variable naming, labeling, and documentation. While ideal is not always the norm, it’s never too late to start building one. Even midway through a project, a thoughtfully crafted data dictionary can bring structure and clarity to your data workflow.\nTraditionally, data dictionaries or codebooks are treated as standalone documentation. But integrating them directly into your data cleaning and transformation pipeline has been a game changer for me. It ensures consistency across renaming, labeling, and dropping operations — all while making your code more readable and reproducible.\n\n\nCreating a new data dictionary\nyou can create a new data dictionary by simply creating a new excel fileby incluing following as column names if you are begining a project.\n\n\n\n\n\n\n\nFields to Include\nOptional Helpful Fields\n\n\n\n\nVariable name\nSkip patterns\n\n\nVariable label (What is this item?)\nRequired item (Were participants allowed to skip this item?)\n\n\nVariable type\nVariable universe (Who got this item?)\n\n\nAllowable values/range\nNotes (such as versions/changes to this variable)\n\n\nAssigned missing values\nAssociated scale/subscale\n\n\nRecoding/calculations\nTime periods this item is available (if study is longitudinal)\n\n\n\nCredits: Crystal Lewis (Data Management in Educational Research)\nI started integrating a data dictionary midway through my project, which meant I had to take a different path. My dataset had over 100 variables, making manual documentation impractical.\nInstead of creating everything from scratch, I began building the dictionary programmatically alongside my cleaning workflow. This approach allowed me to scale efficiently, especially with datasets containing a large number of variables.\nThe code below demonstrates how to create a template data dictionary from an existing dataset:\n\n# Create the data dictionary tibble\ndata_dictionary &lt;- tibble(\n  \"NEW_NAME\" = NA,\n  OLD_NAME = colnames(data),\n  LABEL = NA,\n  VALUE = values,\n  OLD_TYPE = apply(data, 2, typeof)\n)\n\n# Save the updated data dictionary\nwrite_xlsx(data_dictionary,here(\"data_dictionary.xlsx\"))\n\nOnce the data dictionary is created, we can begin integrating it into our cleaning workflow.\n\n\nDropping a variable\nLet’s consider the case of dropping variables. Suppose we want to drop the variable age. In the data_dictionary, we simply set the new_name column for age to \"drop\". Then, we use the following code to filter those variables:\n\ndict &lt;- read_excel(here(\"data_dictionary.xlsx\")) # read the data dictionary\n\ndrop_vars &lt;- dict %&gt;%\n    filter(new_name == \"drop\") %&gt;%\n    pull(old_name)\n\ndata  &lt;- data  |&gt; \nselect(-any_of(drop_vars))\n\nWhile manually tagging new_name as “drop” in the data dictionary might seem tedious at first, it actually helps reduce typos and eliminates the need to repeat variable names throughout your script. Once it’s there in the dictionary, you don’t need to touch it again."
  },
  {
    "objectID": "blog/Data/index.html#renaming-a-variable",
    "href": "blog/Data/index.html#renaming-a-variable",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Renaming a variable",
    "text": "Renaming a variable\nRenaming variables can quickly turn into a nightmare when working with a large number of them. Manually typing both old and new variable names inside the rename() function is not only tedious — it’s also error-prone.\nThis is where the data_dictionary saves the day. Instead of renaming variables directly in your script, you simply enter the new variable names in the new_name column of your dictionary. Then, use a similar approach as we did with dropping variables.\n\n rename_vars &lt;- data_dictionary %&gt;%\n    select(new_name, old_name) %&gt;%\n    filter(!new_name == \"drop\") %&gt;%\n    deframe()\n\ndata &lt;- data |&gt; \nrename(any_of(rename_vars))\n\n\nLabelling variables\nAdding metadata in the form of variable labels was not even in my workflow untill recently and I belive it is common with most R users to not include lables — but trust me, it can make your life a lot easier. With labels in place, you don’t even have to open your data dictionary to get what a column means.it is quite helpful, especially when you’re revisiting a dataset after a while or sharing it with collaborators.\nIn her blog post on labelled data, Shannon Pileggi talks about how labelled data makes it easier to use metadata in tables and figures, which is great if you’re building data products.\nThere are multiple ways to do this in R, but my personal favorite is the codebook package along with labelled package.\nBelow is how I label variables using data dictionay and the codebook package.\n\nvar_labels &lt;- dict |&gt; \nselect(new_name,label) %&gt;% \n  filter(new_name!=\"drop\") %&gt;% \n  dict_to_list()\n\nvar_label(data) &lt;- var_labels\n\n\nIs that all? No!! We can go further — we can create value labels, extend this workflow to auto-generate a codebook, and even integrate it with your reporting pipeline.\nI’ll dive into all that in some later posts.\nFor now — that’s it. Hope this gave you a helpful starting point for making your data cleaning process more structured and reproducible.\nTill next time — bye!! 👋"
  },
  {
    "objectID": "blog/Data/index.html#references",
    "href": "blog/Data/index.html#references",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "References",
    "text": "References\n\nCrystal Lewis. Data Management in Educational Research. https://datamgmtinedresearch.com/\nCrystal Lewis. Creating a data dictionary (Tutorial).\nShannon Pileggi. Working with Labelled Data.\nJenny Bryan. Naming Things — on the importance of naming and organizing code and data files.\nJenny Bryan et al. Happy Git and GitHub for the useR. https://happygitwithr.com/\nlabelled package documentation"
  },
  {
    "objectID": "blog/Data/index.html#disclaimer",
    "href": "blog/Data/index.html#disclaimer",
    "title": "Streamlining R Workflows: A Practitioner’s Guide to Data Management",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis post was drafted with the assistance of AI copy-editing tools to enhance clarity and flow. Any remaining errors are entirely my own."
  },
  {
    "objectID": "blog/LLM/index.html",
    "href": "blog/LLM/index.html",
    "title": "LLMs in R and Python",
    "section": "",
    "text": "R/Python users\nsome experience with coding\nused chatGPT/Claude in browser\nhave not used LLMs in code"
  },
  {
    "objectID": "blog/LLM/index.html#setting-up",
    "href": "blog/LLM/index.html#setting-up",
    "title": "LLMs in R and Python",
    "section": "Setting Up",
    "text": "Setting Up\nWe will use ellmer and dotenv packages in R to set things up. We use dotenv package to load our API keys.\n\ninstall.packages(c(\"ellmer\",\"dotenv\"))\n\nFirst we need to obtain API keys. Since OpenAI and Claude are not free, we will use “Gemini” and “Grok” in our examples\n\nobtain API keys and save in a .env file in the working directory as a key-value pair.\n\nGEMINI_API_KEY=your_key_here\nGROK_API_KEY=your_key_here\n\n# options(ellmer.model = ellmer::chat_groq())\nlibrary(ellmer)\nlibrary(dotenv)\noptions(ellmer.model = ellmer::chat_google_gemini())\n\nNow let us set the model and invoke the chat\n\n# chat_google_gemini(system_prompt = \"You are an experienced R Programmer. Give me only tidyverse codes. Also use proper linting and styling when giving codes\")\nclient &lt;- chat_google_gemini()\n\nUsing model = \"gemini-2.0-flash\".\n\n\nNow that we have invoked the chat, let us prompt something\n\nclient$chat(\"Summarize the plot of Romeo and Juliet in 20 words or less.\")\n\nTwo young lovers from feuding families, Romeo and Juliet, secretly marry and \ntragically kill themselves due to misunderstandings.\n\n\nWe can continue the conversation by calling the chat() again.\n\nclient$chat(\"Now Macbeth\")\n\nDriven by ambition and prophecy, Macbeth murders his way to the throne, but \nguilt and paranoia lead to his downfall and death.\n\n\nNow we can modify the system prompt and get better results\n\nchat_google_gemini(\n  system_prompt = \"You are well versed in literature. Give some lucid explanation of the literature in about 100 words\"\n)\n\n&lt;Chat Google/Gemini/gemini-2.0-flash turns=1 tokens=0/0 $0.00&gt;\n── system [0] ──────────────────────────────────────────────────────────────────\nYou are well versed in literature. Give some lucid explanation of the literature in about 100 words\n\nclient &lt;- chat_google_gemini()\nclient$chat(\"Summarize the plot of Romeo and Juliet\")\n\nRomeo and Juliet is a tragic love story about two young people from feuding \nfamilies in Verona, Italy. Romeo Montague and Juliet Capulet meet at a party \nand instantly fall in love. Unaware of each other's family affiliations, they \nare devastated to learn they are enemies. Despite this, they secretly pledge \ntheir love and are married by Friar Laurence, hoping their union will end the \nfamily feud.\n\nHowever, their bliss is short-lived. Romeo is banished from Verona after \nkilling Juliet's cousin, Tybalt, in a duel to avenge the death of his friend \nMercutio. Juliet's parents, unaware of her marriage to Romeo, arrange for her \nto marry Paris. Desperate to avoid this unwanted marriage, Juliet seeks help \nfrom Friar Laurence, who devises a plan: she will drink a potion that will make\nher appear dead, and Romeo will be alerted and retrieve her from the family \ntomb.\n\nUnfortunately, the message doesn't reach Romeo in time. He hears that Juliet is\ndead and, heartbroken, returns to Verona. He goes to Juliet's tomb, where he \nencounters Paris and kills him in a duel. Believing Juliet to be truly dead, \nRomeo drinks poison and dies beside her.\n\nJuliet awakens to find Romeo dead. Overwhelmed by grief and despair, she takes \nhis dagger and stabs herself.\n\nThe families arrive at the tomb and, witnessing the tragic outcome of their \nhatred, finally reconcile. They vow to honor Romeo and Juliet, acknowledging \nthe devastating consequences of their feud. The play ends with a solemn \nreminder of the cost of hatred and the power of love.\n\n\nwe can also use the interactive window for conversation.\n\nlive_browser(client)\n\nFurther,we can also save results of our chat.\n\nclient &lt;- chat_groq(\n  system_prompt = \"You are a friendly but terse assistant. You always use markdown syntax for code.\",\n)\n\n\n\n\n\n\n\n\nReply from chat using elmer\n\n\n\n\nUsing map Function in R\nThe map function is part of the purrr package in R, which provides a lot of functional programming tools. You can use map to apply a function to each element of a list or vector.\nExample\nlibrary(purrr)\n\n# Create a list of numbers\nnumbers &lt;- 1:5\n\n# Use map to square each number\nmap(numbers, ~ .x^2)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 16\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 25\n\n\nComparison with lapply\nlapply is another function in R that applies a function to each element of a list or vector. Here’s an example:\nExample\nnumbers &lt;- 1:5\n\n# Use lapply to square each number\nlapply(numbers, function(x) x^2)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 16\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 25\nAs you can see, both map and lapply produce the same output. The main difference is that map is more concise and expressive, while lapply is more verbose but provides more control over the iteration process.\nIn general, if you need to perform a simple operation on each element of a collection, map is a good choice. If you need more control or flexibility, lapply might be a better option."
  },
  {
    "objectID": "blog/LLM/index.html#using-map-function-in-r",
    "href": "blog/LLM/index.html#using-map-function-in-r",
    "title": "LLMs in R and Python",
    "section": "Using map Function in R",
    "text": "Using map Function in R\nThe map function is part of the purrr package in R, which provides a lot of functional programming tools. You can use map to apply a function to each element of a list or vector.\nExample\nlibrary(purrr)\n\n# Create a list of numbers\nnumbers &lt;- 1:5\n\n# Use map to square each number\nmap(numbers, ~ .x^2)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 16\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 25"
  },
  {
    "objectID": "blog/LLM/index.html#comparison-with-lapply",
    "href": "blog/LLM/index.html#comparison-with-lapply",
    "title": "LLMs in R and Python",
    "section": "Comparison with lapply",
    "text": "Comparison with lapply\nlapply is another function in R that applies a function to each element of a list or vector. Here’s an example:\nExample\nnumbers &lt;- 1:5\n\n# Use lapply to square each number\nlapply(numbers, function(x) x^2)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 16\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 25\nAs you can see, both map and lapply produce the same output. The main difference is that map is more concise and expressive, while lapply is more verbose but provides more control over the iteration process.\nIn general, if you need to perform a simple operation on each element of a collection, map is a good choice. If you need more control or flexibility, lapply might be a better option."
  },
  {
    "objectID": "blog/LLM/index.html#export-chat",
    "href": "blog/LLM/index.html#export-chat",
    "title": "LLMs in R and Python",
    "section": "Export chat",
    "text": "Export chat\nEasily get a full markdown or HTML export of a conversation:\n\nchat.export(\"convo.md\", title=\"Python Q&A\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nithin M",
    "section": "",
    "text": "Hello, I’m Nithin. Welcome to my website! I’m an economist specializing in macroeconomics, with a passion for data analysis and programming.\nCurrently, I am a Research Assistant at State Planning Board, Government of Kerala\nBeyond research and coding, I’m also an endurance athlete, always pushing boundaries—whether in economic modeling or on the track."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Work in Progress\nDoes Sentiment Predict Consumption Growth of Indian Households? [Slides] [Paper]\nThis is a joint work with Dr. Siddhartha Chattopadhyay of IIT Kharagpur and Dr. Sohini Sahu of IIT Kanpur. Earlier version of this paper was presented at the 19th Annual Conference at ISI Delhi and Workshop on Open Economy Macroeconomics and Economic Integration in Emerging Market Economies at Centre for Development Studies, Thiruvananthapuram.\n\n\nPublications\n\nMani, N., Mishra, A. K., & Pandikasala, J. (2023). How serious is India’s nonperforming assets crisis? A structural satellite version of the financial-macroeconometric model. Asia-Pacific Financial Markets, 30(4), 761–794. https://doi.org/10.1007/s10690-023-09397-9\n\nJijin, P., Mishra, A. K., & Nithin, M. (2022). Macroeconomic determinants of remittances to India. Economic Change and Restructuring, 55(2), 1229–1248.\n\nPandikasala, J., Vyas, I., & Mani, N. (2022). Do financial development drive remittances?\n\nNithin, M., Jijin, P., & Baiju, P. (2018). Has demonetisation pushed digitalisation in India? Some counter evidences. Journal of Business Thought, 9, 58–69. https://doi.org/10.18311/jbt/2018/21170\n\n\n\nResearch Interests\nConsumption Dynamics, Consumption Heterogeneity, Fiscal and Monetary Issues"
  },
  {
    "objectID": "blog/Data/speed.html",
    "href": "blog/Data/speed.html",
    "title": "Testing Python vs R",
    "section": "",
    "text": "As data analysts and researchers, we often toggle between R and Python. But how do they compare in terms of raw speed—especially for common tasks like grouped aggregation?\nIn this post, I benchmark different approaches using both R and Python, including base R, tidyverse, data.table, pandas, and polars. I also experiment with calling Python’s Polars from within R using reticulate, just to see if inter-language calls come with performance penalties—or surprises.\nLet’s dive in."
  },
  {
    "objectID": "blog/Data/speed.html#the-setup",
    "href": "blog/Data/speed.html#the-setup",
    "title": "Testing Python vs R",
    "section": "The Setup",
    "text": "The Setup\nThe dataset I’m using is nba_all_elo.csv. You can get the data here It contains Elo ratings and predictions for thousands of NBA games. This data has 126314 rows and 23 columns.\n\nBase- RR - TidyversePython - PolarsPython - PandasPolars in RPolars inside RR - data.table\n\n\n\ntic()\nnba_base &lt;- read.csv(\"nba_all_elo.csv\")\n\nresult &lt;- aggregate(forecast ~ game_result, data = nba_base, FUN = mean)\nnames(result)[2] &lt;- \"avg_points\"\ntoc()\n\n2.62 sec elapsed\n\n\n\n\n\ntic()\nnba_r &lt;- read_csv(\"nba_all_elo.csv\")\n\nRows: 126314 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): game_id, lg_id, date_game, team_id, fran_id, opp_id, opp_fran, gam...\ndbl (13): gameorder, _iscopy, year_id, seasongame, is_playoffs, pts, elo_i, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnba_r |&gt;\n  summarise(avg_points = mean(forecast), .by = \"game_result\")\n\n# A tibble: 2 × 2\n  game_result avg_points\n  &lt;chr&gt;            &lt;dbl&gt;\n1 L                0.407\n2 W                0.593\n\ntoc()\n\n1.25 sec elapsed\n\n\n\n\n\nstart = time.perf_counter()\ndf = pl.read_csv(\"nba_all_elo.csv\")\nresult = df.group_by(\"game_result\").agg(pl.col(\"forecast\").mean().alias(\"avg_points\"))\nresult\n\n\nshape: (2, 2)\n\n\n\ngame_result\navg_points\n\n\nstr\nf64\n\n\n\n\n\"L\"\n0.407462\n\n\n\"W\"\n0.592538\n\n\n\n\n\nend = time.perf_counter()\n\nprint(f\"Elapsed time: {end - start:.6f} seconds\")\n\nElapsed time: 0.325369 seconds\n\n\n\n\n\nstart = time.perf_counter()\ndf = pd.read_csv(\"nba_all_elo.csv\")\nresult = df.groupby(\"game_result\", as_index=False)[\"forecast\"].mean()\nresult.rename(columns={\"forecast\": \"avg_points\"}, inplace=True)\nend = time.perf_counter()\n\nprint(f\"Elapsed time: {end - start:.6f} seconds\")\n\nElapsed time: 0.541713 seconds\n\n\n\n\n\ntic()\nnba_p &lt;- pl$read_csv(\"nba_all_elo.csv\")\nnba_p$group_by(pl$col(\"game_result\"))$agg(\n  pl$col(\"forecast\")$mean()$alias(\"avg_points\")\n)\n\nshape: (2, 2)\n┌─────────────┬────────────┐\n│ game_result ┆ avg_points │\n│ ---         ┆ ---        │\n│ str         ┆ f64        │\n╞═════════════╪════════════╡\n│ L           ┆ 0.407462   │\n│ W           ┆ 0.592538   │\n└─────────────┴────────────┘\n\ntoc()\n\n0.32 sec elapsed\n\n\n\n\n\n# This is Python code inside R\npy_run_string(\n  \"\nstart = time.perf_counter()\nimport polars as pl\nnba_data = pl.read_csv('nba_all_elo.csv')\nresult = nba_data.group_by('game_result').agg(\n    pl.col('forecast').mean().alias('avg_points')\n)\nresult\nend = time.perf_counter()\n\nprint(f'Elapsed time: {end - start:.6f} seconds')\n\"\n)\n\nElapsed time: 0.046371 seconds\n\n\n\n\n\ntic()\n# Read CSV and convert to data.table\nnba_dt &lt;- fread(\"nba_all_elo.csv\")\n\n# Grouped aggregation\nresult &lt;- nba_dt[,\n  .(avg_points = mean(forecast, na.rm = TRUE)),\n  by = game_result\n]\ntoc()\n\n0.34 sec elapsed"
  },
  {
    "objectID": "blog/Data/speed.html#key-takeaways",
    "href": "blog/Data/speed.html#key-takeaways",
    "title": "Testing Python vs R",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nHere’s what stood out from the benchmarks:\n✅ Polars in R is the fastest option available. data.table is fast option but often comes at the cost of syntax which I am not comfortable with. It comfortably outsmarts both base R and the tidyverse.\n✅ Polars is blazingly fast often 5–10x faster than Pandas or Tidyverse.\n✅ Surprise! When I invoked Python’s Polars from within R via reticulate, it was marginally faster than running the same code directly in Python. This may be due to JIT caching or differences in Quarto execution context—but it’s worth investigating further."
  },
  {
    "objectID": "blog/Data/speed.html#final-thoughts",
    "href": "blog/Data/speed.html#final-thoughts",
    "title": "Testing Python vs R",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nSo, if performance is your priority:\n\nUse Polars in R.\nUse polars in Python—or even inside R if you’re already mixing languages.\nAvoid tidyverse for speed-critical tasks, unless readability is your goal.\n\nWith tools like reticulate and quarto, we can blend strengths across ecosystems—without giving up speed. These tools help us to get the best of both worlds.\nThis document was built with R version 4.4.3 and Python version 3.11.\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nR\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.3 (2025-02-28 ucrt)\n os       Windows 11 x64 (build 26100)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_India.utf8\n ctype    English_India.utf8\n tz       Asia/Calcutta\n date     2025-06-07\n pandoc   3.6.2 @ C:/Users/NITHIN~1/AppData/Local/Pandoc/ (via rmarkdown)\n quarto   1.6.40 @ C:\\\\PROGRA~1\\\\Quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n data.table  * 1.15.4  2024-03-30 [1] CRAN (R 4.4.1)\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.4.1)\n pacman      * 0.5.1   2019-03-11 [1] CRAN (R 4.4.3)\n polars      * 0.22.4  2025-05-31 [1] https://r-multiverse.r-universe.dev (R 4.4.3)\n quarto      * 1.4.4   2024-07-20 [1] CRAN (R 4.4.1)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.4.1)\n reticulate  * 1.42.0  2025-03-25 [1] CRAN (R 4.4.3)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.4.1)\n tictoc      * 1.2.1   2024-03-18 [1] CRAN (R 4.4.1)\n tidypolars  * 0.13.0  2025-05-28 [1] https://r-multiverse.r-universe.dev (R 4.4.3)\n\n [1] C:/Users/Nithin M/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.3/library\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Scripts/python.exe\n libpython:      C:/Users/Nithin M/AppData/Roaming/uv/python/cpython-3.11.11-windows-x86_64-none/python311.dll\n pythonhome:     C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv\n virtualenv:     C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Scripts/activate_this.py\n version:        3.11.11 (main, Feb 12 2025, 14:49:02) [MSC v.1942 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Lib/site-packages/numpy\n numpy_version:  2.2.6\n \n NOTE: Python version was forced by py_require()\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\nPython\n\n\npandas: 2.3.0\n\n\npolars: 1.30.0"
  },
  {
    "objectID": "blog/Data/speed.html#r",
    "href": "blog/Data/speed.html#r",
    "title": "Testing Python vs R",
    "section": "R",
    "text": "R\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.3 (2025-02-28 ucrt)\n os       Windows 11 x64 (build 26100)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_India.utf8\n ctype    English_India.utf8\n tz       Asia/Calcutta\n date     2025-06-07\n pandoc   3.6.2 @ C:/Users/NITHIN~1/AppData/Local/Pandoc/ (via rmarkdown)\n quarto   1.6.40 @ C:\\\\PROGRA~1\\\\Quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n data.table  * 1.15.4  2024-03-30 [1] CRAN (R 4.4.1)\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.4.1)\n pacman      * 0.5.1   2019-03-11 [1] CRAN (R 4.4.3)\n polars      * 0.22.4  2025-05-31 [1] https://r-multiverse.r-universe.dev (R 4.4.3)\n quarto      * 1.4.4   2024-07-20 [1] CRAN (R 4.4.1)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.4.1)\n reticulate  * 1.42.0  2025-03-25 [1] CRAN (R 4.4.3)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.4.1)\n tictoc      * 1.2.1   2024-03-18 [1] CRAN (R 4.4.1)\n tidypolars  * 0.13.0  2025-05-28 [1] https://r-multiverse.r-universe.dev (R 4.4.3)\n\n [1] C:/Users/Nithin M/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.3/library\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Scripts/python.exe\n libpython:      C:/Users/Nithin M/AppData/Roaming/uv/python/cpython-3.11.11-windows-x86_64-none/python311.dll\n pythonhome:     C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv\n virtualenv:     C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Scripts/activate_this.py\n version:        3.11.11 (main, Feb 12 2025, 14:49:02) [MSC v.1942 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/Users/Nithin M/OneDrive/Documents/GitHub/Websites and CV/websites/personal/nithinmkp.github.io/.venv/Lib/site-packages/numpy\n numpy_version:  2.2.6\n \n NOTE: Python version was forced by py_require()\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "blog/Data/speed.html#python",
    "href": "blog/Data/speed.html#python",
    "title": "Testing Python vs R",
    "section": "Python",
    "text": "Python\n\n\npandas: 2.3.0\n\n\npolars: 1.30.0"
  }
]